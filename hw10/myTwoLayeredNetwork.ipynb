{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural & Behavioral Modeling - Week 10 (Exercises)\n",
    "by 駱皓正(austenpsy@gmail.com)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%config IPCompleter.greedy=True \n",
    "%matplotlib inline\n",
    "from numpy import *\n",
    "from matplotlib.pyplot import *\n",
    "from IPython.display import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 A Two-layered Linear Network as a Regression Model (7 points)\n",
    "Data fitting of the following network is poor. Please check if adding bias terms or chaging network hyperparameters (e.g., learning rate, amount of training, etc.) help. If not, please explain why the fitting is poor given that the network/regression model has sufficient degrees of freedom (i.e., network weights or regression coefficients) to overfit such a small data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ideal results:\n",
      "[[1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [0 1]]\n",
      "Reality:\n",
      "[[ 0.679 -0.   ]\n",
      " [ 0.643  0.   ]\n",
      " [ 1.321  0.   ]\n",
      " [-0.     0.643]\n",
      " [ 0.     0.643]\n",
      " [ 0.     1.286]]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAW4AAAEICAYAAAB/Dx7IAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10b59fa58>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Here we train a two-layered network of units \n",
    "# with a linear activation function f(x)=x\n",
    "# to associate patterns using the delta rule dW=(t-y)*x\n",
    "\n",
    "set_printoptions(precision=3,suppress=True)\n",
    "\n",
    "X=array([[1,0,0,0],[0,1,0,0],[1,1,0,0],[0,0,1,0],[0,0,0,1],[0,0,1,1]])\n",
    "Y=array([[1,0],[1,0],[1,0],[0,1],[0,1],[0,1]])\n",
    "[Np,Nx]=X.shape; # find numbers of patterns and input dimensions\n",
    "[Np,Ny]=Y.shape; # find numbers of patterns and output dimensions\n",
    "W=random.rand(Ny,Nx); # set initially random connectivity matrix\n",
    "\n",
    "eta=.1; # set the learning rate \n",
    "tol=1e-2; # set the tolerance/stopping criterion; try 0.01\n",
    "nIts=50000; # set the maximum number of allowed iterations\n",
    "totErr=10; # set the maximum training error to an initially high value\n",
    "totErr_hist=[] # history of totall error\n",
    "\n",
    "for c in range(nIts): # for each learning iteration\n",
    "    p=mod(c,Np) # sequential presentation of the training samples\n",
    "    #p=random.randint(nP); # choose a traing pattern at random\n",
    "    \n",
    "    # Forward propagation:\n",
    "    y=W.dot(X[p])\n",
    "    \n",
    "    # Backward propagation:\n",
    "    deltaW=eta*outer(Y[p].T-y,X[p]) # delta learning\n",
    "    W=W+deltaW;  # apply the weight update\n",
    "    \n",
    "    # Checking if done:\n",
    "    if(mod(c,10*Np)==0): # after 10 updates check total errors\n",
    "        predY=W.dot(X.T) # testing ALL the training samples\n",
    "        totErr=sum((Y.T-predY)**2) # sum of squared errors for all samples\n",
    "        totErr_hist.append(totErr)\n",
    "    if(totErr<tol):\n",
    "        break # break if max error is below tolerance\n",
    "        \n",
    "plot(totErr_hist);\n",
    "title(str(c)+':'+str(totErr));\n",
    "print('Ideal results:')\n",
    "print(Y)\n",
    "print('Reality:')\n",
    "print(predY.T) # predicted Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ideal results:\n",
      "[[1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [0 1]]\n",
      "Reality:\n",
      "[[ 0.835  0.165]\n",
      " [ 0.831  0.169]\n",
      " [ 1.165 -0.165]\n",
      " [ 0.169  0.831]\n",
      " [ 0.17   0.83 ]\n",
      " [-0.162  1.162]]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAEICAYAAABPgw/pAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10f019390>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "### Write your codes with bias terms here\n",
    "# Here we train a two-layered network of units \n",
    "# with a linear activation function f(x)=x\n",
    "# to associate patterns using the delta rule dW=(t-y)*x\n",
    "\n",
    "set_printoptions(precision=3,suppress=True)\n",
    "\n",
    "#training data\n",
    "X=array([[1,0,0,0],[0,1,0,0],[1,1,0,0],[0,0,1,0],[0,0,0,1],[0,0,1,1]])\n",
    "Y=array([[1,0],[1,0],[1,0],[0,1],[0,1],[0,1]])\n",
    "[Np,Nx]=X.shape; # find numbers of patterns and input dimensions\n",
    "#Np = 6; Nx = 4\n",
    "[Np,Ny]=Y.shape; # find numbers of patterns and output dimensions\n",
    "#Np = 6; Ny = 2\n",
    "W=random.rand(Ny,Nx+1); # set initially random connectivity matrix\n",
    "#1 represent bias term\n",
    "\n",
    "eta=.01; # set the learning rate \n",
    "tol=1e-2; # set the tolerance/stopping criterion; try 0.01\n",
    "nIts=50000; # set the maximum number of allowed iterations\n",
    "totErr=10; # set the maximum training error to an initially high value\n",
    "totErr_hist=[] # history of totall error\n",
    "\n",
    "for c in range(nIts): # for each learning iteration\n",
    "    p=mod(c,Np) # sequential presentation of the training samples\n",
    "    #p=random.randint(nP); # choose a traing pattern at random\n",
    "    \n",
    "    # Forward propagation:\n",
    "    x0=append(X[p],1) #adding bias term\n",
    "    y=W.dot(x0)\n",
    "    \n",
    "    # Backward propagation:\n",
    "    deltaW=eta*outer(Y[p].T-y,x0) # delta learning\n",
    "    W=W+deltaW;  # apply the weight update\n",
    "    \n",
    "    # Checking if done:\n",
    "    if(mod(c,10*Np)==0): # after 10 updates check total errors\n",
    "        x=hstack([X,ones([Np,1])]) # append bias to all input patterns\n",
    "        predY=W.dot(x.T) # testing ALL the training samples\n",
    "        totErr=sum((Y.T-predY)**2) # sum of squared errors for all samples\n",
    "        totErr_hist.append(totErr)\n",
    "    if(totErr<tol):\n",
    "        break # break if max error is below tolerance\n",
    "        \n",
    "plot(totErr_hist);\n",
    "title(str(c)+':'+str(totErr));\n",
    "print('Ideal results:')\n",
    "print(Y)\n",
    "print('Reality:')\n",
    "print(predY.T) # predicted Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Discussions\n",
    "\n",
    "Given the original set of hyperparameters, after the bias term added, the total error decreased in 50% (from 0.670 to 0.345). Letting learning rate to be 1/10 of original, total error decreased (from 0.345 to 0.333). The effects of other hyperparameter has few impacts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 PyTorch (3 points)\n",
    "Read <a href=\"http://noahsnail.com/2017/09/18/2017-9-18-PyTorch%E5%9F%BA%E6%9C%AC%E7%94%A8%E6%B3%95(%E4%B8%80)%E2%80%94%E2%80%94Numpy%EF%BC%8CTorch%E5%AF%B9%E6%AF%94/\">this tutorial</a> first and port the following Instar Learning from NumPy to PyTorch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.48145628  0.17285606  0.26581023] 0.232907559369\n",
      "[ 0.39261223  0.20246885  0.32035479] 0.260179273937\n",
      "[ 0.31648059  0.22784443  0.36709475] 0.283548765228\n",
      "[ 0.25509779  0.24830405  0.40477987] 0.302390930796\n",
      "[ 0.20819762  0.26393644  0.43357358] 0.316787481547\n",
      "[ 0.17392197  0.27536092  0.45461663] 0.327308791623\n",
      "[ 0.14972666  0.28342551  0.46947101] 0.334735823618\n",
      "[ 0.13308137  0.28897359  0.47969016] 0.339845290271\n",
      "[ 0.12183882  0.29272086  0.48659236] 0.343296320546\n",
      "[ 0.11434163  0.29521976  0.49119515] 0.345597669037\n"
     ]
    }
   ],
   "source": [
    "# Instar learning:\n",
    "x=array([0.1,0.3,0.5])\n",
    "W=random.rand(3)\n",
    "for i in range(10): # trials \n",
    "    y=dot(W,x) \n",
    "    print(W,y)\n",
    "    W+=y*(x-W) # Postsynaptically gated InStar "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.1094,  0.2969,  0.4942], dtype=torch.float64) tensor(0.3471, dtype=torch.float64)\n",
      "tensor([ 0.1061,  0.2980,  0.4962], dtype=torch.float64) tensor(0.3481, dtype=torch.float64)\n",
      "tensor([ 0.1040,  0.2987,  0.4975], dtype=torch.float64) tensor(0.3488, dtype=torch.float64)\n",
      "tensor([ 0.1026,  0.2991,  0.4984], dtype=torch.float64) tensor(0.3492, dtype=torch.float64)\n",
      "tensor([ 0.1017,  0.2994,  0.4990], dtype=torch.float64) tensor(0.3495, dtype=torch.float64)\n",
      "tensor([ 0.1011,  0.2996,  0.4993], dtype=torch.float64) tensor(0.3497, dtype=torch.float64)\n",
      "tensor([ 0.1007,  0.2998,  0.4996], dtype=torch.float64) tensor(0.3498, dtype=torch.float64)\n",
      "tensor([ 0.1005,  0.2998,  0.4997], dtype=torch.float64) tensor(0.3499, dtype=torch.float64)\n",
      "tensor([ 0.1003,  0.2999,  0.4998], dtype=torch.float64) tensor(0.3499, dtype=torch.float64)\n",
      "tensor([ 0.1002,  0.2999,  0.4999], dtype=torch.float64) tensor(0.3499, dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "### Write your PyTorch codes here\n",
    "import torch as t\n",
    "\n",
    "xt = t.from_numpy(x)\n",
    "H = t.from_numpy(W)\n",
    "for i in range(10): # trials \n",
    "    y=t.dot(H,xt) \n",
    "    print(H,y)\n",
    "    H+=y*(x-H) # Postsynaptically gated InStar "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
